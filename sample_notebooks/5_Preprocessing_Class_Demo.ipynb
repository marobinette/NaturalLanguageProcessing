{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#lowecasing"
      ],
      "metadata": {
        "id": "VrcQmUlieGNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3ldEry5dWNg",
        "outputId": "771bb684-0a0b-415c-9c11-aa53c095e3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: General Motors is BIGGER than general motors.\n",
            "Lower: general motors is bigger than general motors.\n",
            "Casefold: general motors is bigger than general motors.\n"
          ]
        }
      ],
      "source": [
        "# Simple example of lowercasing text in Python\n",
        "\n",
        "text = \"General Motors is BIGGER than general motors.\"\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Lower:\", text.lower())       # basic lowercase\n",
        "print(\"Casefold:\", text.casefold()) # more aggressive, handles accents\n",
        "# Tip: .casefold() is better for multilingual text\n",
        "# because it handles more Unicode cases (e.g., German “ß” → “ss”\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HTML Strip"
      ],
      "metadata": {
        "id": "oMW58Jk9eQlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, html"
      ],
      "metadata": {
        "id": "QBLMShwre8UQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pure stdlib (regex + unescape) — tiny, but brittle on messy HTML\n",
        "s = '<p>Hello <b>world</b> &amp; <a href=\"#\">friends</a>!</p>'\n",
        "text = re.sub(r'<[^>]+>', '', s)        # strip tags\n",
        "text = html.unescape(text).strip()      # decode entities like &amp; -> &\n",
        "print(text)  # Hello world friends!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_yrB3g6eS6S",
        "outputId": "a3f99ad6-ae20-4cd7-d205-b7a896689ea1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world & friends!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "cGglCcttfK-w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = '<p>Hello <b>world</b> &amp; <a href=\"#\">friends</a>! <script>alert(1)</script></p>'\n",
        "text = BeautifulSoup(s, 'html.parser').get_text(\" \", strip=True)\n",
        "print(text)  # Hello world friends!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG-ZLO3BfOPj",
        "outputId": "2b1ded9f-e0a8-4a30-c2dc-6b1e193ca8d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world & friends !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Strip Punctuation"
      ],
      "metadata": {
        "id": "hBk22K4RfR9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "oY-2slstfWCl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world! NLP is fun... right?\"\n",
        "clean = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(clean)  # Hello world NLP is fun right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8cvPQq3fi5p",
        "outputId": "028c40e1-1a41-43ad-8369-2d9c7111b80c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world NLP is fun right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "nk8hX2Nqfrn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "XtV9liKigBx0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "stems = [stemmer.stem(w) for w in words]\n",
        "print(stems)  # ['run', 'runner', 'run', 'easili', 'fairli']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQfSOUkygAI5",
        "outputId": "10058bbc-d660-46f7-dd7a-f9bd5720c299"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'run', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatizing"
      ],
      "metadata": {
        "id": "Kn_pf2U2gInE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "7UJ7Xf_0gRyj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e842040",
        "outputId": "b9198391-955b-442d-9ca6-642313717cb1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"runs\", \"better\", \"geese\"]\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
        "print(lemmas)  # ['running', 'run', 'better', 'goose']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMKo9ml2gfV8",
        "outputId": "6b0e9cf8-3ab5-4036-89a2-3552dff7a476"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'run', 'better', 'goose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "H4yVpch9olhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "-z-SWPaEgzsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffikbc0FgWv2",
        "outputId": "af49d43d-b5b1-440a-e8dc-9cf07d2ce119"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a simple example showing off stop word filtration.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "filtered = [w for w in tokens if w.lower() not in stopwords.words(\"english\")]\n",
        "print(filtered)\n",
        "# ['This', 'simple', 'example', 'showing', 'stop', 'word', 'filtration', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a5qMna5hTP9",
        "outputId": "0f31280c-a329-44fe-ae3d-6e328ec6151a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sentence segmentation"
      ],
      "metadata": {
        "id": "6mA2cq26o5vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"The cat sat on the mat. The dog barked loudly. And then it ran away!\"\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZxkpuwso7Xy",
        "outputId": "908efa2f-b6c3-430e-d81a-1a521c5849f0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The cat sat on the mat.', 'The dog barked loudly.', 'And then it ran away!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "S_uApBD-hUDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple whitespace tokenizer**"
      ],
      "metadata": {
        "id": "oUqurr0bkOId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Whitespace tokenization\n",
        "# Example sentence\n",
        "text = \"The runners were running quickly in the U.S.A.\"\n",
        "tokens_ws = text.split()\n",
        "print(\"Whitespace:\", tokens_ws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6wQsUafiDbB",
        "outputId": "a41ce134-05fa-40e5-9313-8c1d2cfeaa63"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace: ['The', 'runners', 'were', 'running', 'quickly', 'in', 'the', 'U.S.A.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word_tokenize in NLTK** is a wrapper around the TreebankWordTokenizer, based on the Penn Treebank conventions.\n",
        "\n",
        "It applies a series of regular-expression rules to split text into tokens.\n",
        "\n",
        "What it does step by step\n",
        "\n",
        "Whitespace splitting → starts by splitting on spaces.\n",
        "\n",
        "* Punctuation separation → separates most punctuation from words. \"Let's go!\" → [\"Let\", \"'s\", \"go\", \"!\"]\n",
        "\n",
        "*  Contractions & clitics → splits common English contractions. \"can't\" → [\"ca\", \"n't\"], \"he'll\" → [\"he\", \"'ll\"]\n",
        "\n",
        "* Special cases → keeps some tokens intact. \"U.S.A.\" stays as \"U.S.A.\" Numbers with decimals \"3.14\" stay whole.\n",
        "\n",
        "* Quotes handling → normalizes quotation marks to opening/closing forms."
      ],
      "metadata": {
        "id": "Va_hV8j8kNGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. NLTK word_tokenize\n",
        "# Example sentence\n",
        "text = \"The runners were running quickly in the U.S.A.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens_nltk = word_tokenize(text)\n",
        "print(\"NLTK:\", tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ql3Zt39iEut",
        "outputId": "f1cf60a2-4a9d-417d-ccab-236499c8158c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK: ['The', 'runners', 'were', 'running', 'quickly', 'in', 'the', 'U.S.A', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE (Byte Pair Encoding) tokenization**\n",
        "\n",
        "* Start with characters as the smallest units.\n",
        "\n",
        "* Find the most frequent pair of symbols (letters, or subwords) in the text.\n",
        "\n",
        "* Merge that pair into a new token.\n",
        "\n",
        "* Repeat until you reach the desired vocabulary size."
      ],
      "metadata": {
        "id": "mfas5Esxk7MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "text = \"The runners were running quickly in the U.S.A.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC8JtPrziGRp",
        "outputId": "89d648ab-b4e6-4d78-8162-571af52beb25"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Ġrunners', 'Ġwere', 'Ġrunning', 'Ġquickly', 'Ġin', 'Ġthe', 'ĠU', '.', 'S', '.', 'A', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does Ġ mean in BPE?**\n",
        "\n",
        "* Ġ = marker for a space before a word\n",
        "\n",
        "* Example: \"Hello world\" → ['Hello', 'Ġworld']\n",
        "\n",
        "* \"world\" at start of sentence → ['world']\n",
        "\n",
        "* Keeps spaces explicit so the model can:\n",
        "\n",
        "* Reconstruct the original text\n",
        "\n",
        "* Distinguish \"world\" vs. \" Ġworld\"\n",
        "\n",
        "* (Other tokenizers use different markers — e.g. SentencePiece uses ▁ for space.)"
      ],
      "metadata": {
        "id": "9dKb_y3Dly6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words"
      ],
      "metadata": {
        "id": "zE5_-y2Il_uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog sat on the log.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFermDrZmCCh",
        "outputId": "5437d133-dcc1-43a4-9d31-e65bf5cdf1f1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['cat' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n",
            "BoW Matrix:\n",
            " [[1 0 0 1 1 1 2]\n",
            " [0 1 1 0 1 1 2]]\n"
          ]
        }
      ]
    }
  ]
}